{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Q3_intent_classifier.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecgZefUaFNbu",
        "outputId": "2d97da78-d498-41fb-d7e2-97410e71b50a"
      },
      "source": [
        "#clone the dataset repo\r\n",
        "!git clone https://github.com/clinc/oos-eval.git\r\n",
        "%cd oos-eval"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'oos-eval'...\n",
            "remote: Enumerating objects: 64, done.\u001b[K\n",
            "remote: Counting objects: 100% (64/64), done.\u001b[K\n",
            "remote: Compressing objects: 100% (43/43), done.\u001b[K\n",
            "remote: Total 64 (delta 33), reused 48 (delta 19), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (64/64), done.\n",
            "/content/oos-eval\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M77j8YpJFg-s",
        "outputId": "8bfca676-0eda-41bb-d6bb-7a8cb383c91b"
      },
      "source": [
        "#import necessary packages\r\n",
        "\r\n",
        "import json\r\n",
        "from random import randint\r\n",
        "import numpy as np\r\n",
        "import nltk\r\n",
        "import string\r\n",
        "import re\r\n",
        "import pandas as pd\r\n",
        "from sklearn.utils import shuffle\r\n",
        "from nltk.stem import WordNetLemmatizer\r\n",
        "from nltk.tokenize import word_tokenize\r\n",
        "from keras.preprocessing.text import Tokenizer\r\n",
        "from keras.preprocessing.sequence import pad_sequences\r\n",
        "from sklearn.preprocessing import OneHotEncoder\r\n",
        "from keras.utils import to_categorical\r\n",
        "from keras.models import Sequential, load_model\r\n",
        "from keras.layers import Dense, LSTM, Bidirectional, Embedding, Dropout,BatchNormalization\r\n",
        "from keras.callbacks import ModelCheckpoint\r\n",
        "from random import seed\r\n",
        "import os\r\n",
        "\r\n",
        "nltk.download('wordnet')\r\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ttZrrbJFmBe"
      },
      "source": [
        "#class definition\r\n",
        "class IntentClassifier:\r\n",
        "  def __init__(self): #constructor\r\n",
        "    self.data = {}\r\n",
        "    self.random_20_classes = []\r\n",
        "    self.train_data = {'text':[],'intent':[]}\r\n",
        "    self.test_data = {'text':[],'intent':[]}\r\n",
        "    self.val_data = {'text':[],'intent':[]}\r\n",
        "    self.X_train = None\r\n",
        "    self.X_test = None\r\n",
        "    self.X_val = None \r\n",
        "    self.Y_train = None\r\n",
        "    self.Y_test = None\r\n",
        "    self.Y_val = None\r\n",
        "    self.unique_intent = []\r\n",
        "    self.lemmatizer = WordNetLemmatizer()\r\n",
        "    self.word_tokenizer = Tokenizer()\r\n",
        "    self.Xtrain_clean = []\r\n",
        "    self.Xval_clean = []\r\n",
        "    self.vocab_size = 0\r\n",
        "    self.max_length = 0\r\n",
        "    self.padded_train = None\r\n",
        "    self.padded_val = None\r\n",
        "    self.encoded_input_train = None\r\n",
        "    self.encoded_val = None\r\n",
        "    self.output_tokenizer = None\r\n",
        "    self.encoded_output_train = None\r\n",
        "    self.encoded_output_val = None\r\n",
        "    self.output_onehot_train = None\r\n",
        "    self.output_onehot_val = None\r\n",
        "    self.model = None\r\n",
        "    self.best_model_name = 'intent_classification_model.h5'\r\n",
        "  \r\n",
        "  #read the json file\r\n",
        "  def read_file(self,filename):\r\n",
        "    current_directory = os.getcwd()\r\n",
        "    file_directory = current_directory+'/'+filename\r\n",
        "    with open(file_directory) as f:\r\n",
        "      self.data = json.load(f)\r\n",
        "    print('keys :')\r\n",
        "    #classes in the json file\r\n",
        "    for key in self.data.keys():\r\n",
        "      print(key)\r\n",
        "\r\n",
        "  #load dataset as X and Y values \r\n",
        "  def load_dataset(self,load_data):\r\n",
        "    load_data = pd.DataFrame(load_data)\r\n",
        "    X = load_data['text']\r\n",
        "    Y = load_data['intent']\r\n",
        "    unique_intent = list(set(Y))\r\n",
        "    return (X,Y,unique_intent) \r\n",
        "  \r\n",
        "  #text cleaning step - remove punctuation, tokenize and lemmatize\r\n",
        "  def cleaning(self,sentences):\r\n",
        "    words = []\r\n",
        "    for s in sentences:\r\n",
        "      clean = \"\".join([i.lower() for i in s if i not in string.punctuation])\r\n",
        "      w = word_tokenize(clean)\r\n",
        "      #lemmatizing\r\n",
        "      words.append([self.lemmatizer.lemmatize(i) for i in w])\r\n",
        "    return words\r\n",
        "\r\n",
        "  #create encodings\r\n",
        "  def create_tokenizer(self,words,oov,filters = '!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~'):\r\n",
        "    if oov:\r\n",
        "      token = Tokenizer(filters = filters, oov_token='OOV')\r\n",
        "    else:\r\n",
        "      token = Tokenizer(filters = filters)\r\n",
        "    token.fit_on_texts(words)\r\n",
        "    return token\r\n",
        "  \r\n",
        "  #getting maximum length\r\n",
        "  def max_length_func(self,words):\r\n",
        "    return(len(max(words, key = len)))\r\n",
        "  \r\n",
        "  #encoding list of words\r\n",
        "  def encoding_doc(self, token, words):\r\n",
        "    return(token.texts_to_sequences(words))\r\n",
        "\r\n",
        "  #pad the sequence  \r\n",
        "  def padding_doc(self,encoded_doc):\r\n",
        "    return(pad_sequences(encoded_doc, maxlen = self.max_length,padding = \"post\"))\r\n",
        "\r\n",
        "  #one hot encoding \r\n",
        "  def one_hot(self,encode):\r\n",
        "    o = OneHotEncoder(sparse = False)\r\n",
        "    return(o.fit_transform(encode))\r\n",
        "  \r\n",
        "  #data preparation\r\n",
        "  def data_preparation(self,filename):\r\n",
        "    print('Data preparation begins')\r\n",
        "    #seed random number generator\r\n",
        "    seed(1)\r\n",
        "    i = 0\r\n",
        "    self.read_file(filename)\r\n",
        "\r\n",
        "    # generate 20 random integer values\r\n",
        "    while i!=20:\r\n",
        "      value = randint(0, 149)\r\n",
        "      if value not in self.random_20_classes:\r\n",
        "        self.random_20_classes.append(value)\r\n",
        "        i = i+1\r\n",
        "\r\n",
        "    #get the data for train, test and val\r\n",
        "    for ele in self.random_20_classes:\r\n",
        "      for i in range(ele*100,ele*100+100):\r\n",
        "        self.train_data['text'].append(self.data['train'][i][0])\r\n",
        "        self.train_data['intent'].append(self.data['train'][i][1])\r\n",
        "      for i in range(ele*30,ele*30+30):\r\n",
        "        self.test_data['text'].append(self.data['test'][i][0])\r\n",
        "        self.test_data['intent'].append(self.data['test'][i][1])\r\n",
        "      for i in range(ele*20,ele*20+20):\r\n",
        "        self.val_data['text'].append(self.data['val'][i][0])\r\n",
        "        self.val_data['intent'].append(self.data['val'][i][1])\r\n",
        "    \r\n",
        "    # call load dataset to create dataset\r\n",
        "    self.X_train,self.Y_train,self.unique_intent = self.load_dataset(self.train_data)\r\n",
        "    self.X_test,self.Y_test,_ = self.load_dataset(self.test_data)\r\n",
        "    self.X_val,self.Y_val,_ = self.load_dataset(self.val_data)\r\n",
        "\r\n",
        "    #clean train and val\r\n",
        "    self.Xtrain_clean = self.cleaning(self.X_train)\r\n",
        "    self.Xval_clean  = self.cleaning(self.X_val)\r\n",
        "\r\n",
        "    #tokenizer\r\n",
        "    self.word_tokenizer = self.create_tokenizer(self.Xtrain_clean,oov=True)\r\n",
        "\r\n",
        "    #vocab_size\r\n",
        "    self.vocab_size = len(self.word_tokenizer.word_index)+1\r\n",
        "    \r\n",
        "    #max_length calculation\r\n",
        "    self.max_length = self.max_length_func(self.Xtrain_clean)\r\n",
        "\r\n",
        "    #encode train and val sentences\r\n",
        "    self.encoded_input_train = self.encoding_doc(self.word_tokenizer,self.Xtrain_clean)\r\n",
        "    self.encoded_val = self.encoding_doc(self.word_tokenizer,self.Xval_clean)\r\n",
        "\r\n",
        "    #pad train and val sentences\r\n",
        "    self.padded_train = self.padding_doc(self.encoded_input_train)\r\n",
        "    self.padded_val = self.padding_doc(self.encoded_val)\r\n",
        "    \r\n",
        "    #create encodings for output intent class\r\n",
        "    self.output_tokenizer = self.create_tokenizer(self.unique_intent, oov=False,filters = '!\"#$%&()*+,-/:;<=>?@[\\]^`{|}~')\r\n",
        "    print(self.output_tokenizer.word_index)\r\n",
        "\r\n",
        "    #encode output labels for train and val\r\n",
        "    self.encoded_output_train = self.encoding_doc(self.output_tokenizer, self.Y_train)\r\n",
        "    self.encoded_output_val = self.encoding_doc(self.output_tokenizer,self.Y_val)\r\n",
        "\r\n",
        "    #reshape vector\r\n",
        "    self.encoded_output_train = np.array(self.encoded_output_train).reshape(len(self.encoded_output_train), 1)\r\n",
        "    self.encoded_output_val = np.array(self.encoded_output_val).reshape(len(self.encoded_output_val), 1)\r\n",
        "\r\n",
        "    #one hot encode Y for train and val\r\n",
        "    self.output_onehot_train = self.one_hot(self.encoded_output_train)\r\n",
        "    self.output_onehot_val = self.one_hot(self.encoded_output_val)\r\n",
        "  \r\n",
        "  #create model\r\n",
        "  def create_model(self):\r\n",
        "    print('Model Created')\r\n",
        "    model = Sequential()\r\n",
        "    model.add(Embedding(self.vocab_size, 128, input_length = self.max_length))\r\n",
        "    model.add(Bidirectional(LSTM(128)))\r\n",
        "    model.add(Dense(64, activation = \"relu\"))\r\n",
        "    model.add(Dropout(0.4))\r\n",
        "    model.add(Dense(20, activation = \"softmax\"))\r\n",
        "    return model\r\n",
        "\r\n",
        "  def train_model(self,filename):\r\n",
        "    self.data_preparation(filename)\r\n",
        "    self.model = self.create_model()\r\n",
        "    #optimizer, metrics, and loss\r\n",
        "    self.model.compile(loss = \"categorical_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\r\n",
        "    self.model.summary()\r\n",
        "    #checkpoint for model\r\n",
        "    checkpoint = ModelCheckpoint(self.best_model_name, verbose=0, save_best_only=True, mode='auto',save_freq='epoch')\r\n",
        "\r\n",
        "    #shuffle data\r\n",
        "    train_X, train_Y = shuffle(self.padded_train,self.output_onehot_train)\r\n",
        "    val_X,val_Y = shuffle(self.padded_val,self.output_onehot_val)\r\n",
        "    print('Training begins')\r\n",
        "\r\n",
        "    #model training\r\n",
        "    self.model.fit(train_X, train_Y, epochs = 10, batch_size = 32, validation_data = (val_X, val_Y), callbacks = [checkpoint])\r\n",
        "    \r\n",
        "  #calculate predictions \r\n",
        "  def predictions(self,text):\r\n",
        "    test_word = self.cleaning(text)\r\n",
        "    test_enc = self.encoding_doc(self.word_tokenizer,test_word)\r\n",
        "    x = self.padding_doc(test_enc)\r\n",
        "    pred = self.model.predict(x)\r\n",
        "    return pred\r\n",
        "    \r\n",
        "  #calculate classes\r\n",
        "  def get_final_output(self,pred, classes):\r\n",
        "    predictions = pred[0]\r\n",
        "    classes = np.array(classes)\r\n",
        "    ids = np.argsort(-predictions)\r\n",
        "    classes = classes[ids]\r\n",
        "    predictions = -np.sort(-predictions)\r\n",
        "    return classes[0]\r\n",
        "\r\n",
        "  #calculate test accuracy\r\n",
        "  def evaluate_test(self):\r\n",
        "    #load best model\r\n",
        "    self.model = load_model(self.best_model_name)\r\n",
        "    count = 0\r\n",
        "    for i in range(len(self.X_test)):\r\n",
        "      pred = self.predictions([self.X_test[i]])\r\n",
        "      cls = self.get_final_output(pred,self.unique_intent)\r\n",
        "      if cls==self.Y_test[i]:\r\n",
        "        count=count+1\r\n",
        "    print('test accuracy', (count/len(self.Y_test)))\r\n",
        "  \r\n",
        "  #perform demo with one sentence\r\n",
        "  def model_demo(self,text):\r\n",
        "    #load best model\r\n",
        "    self.model = load_model(self.best_model_name)\r\n",
        "    pred = self.predictions([text])\r\n",
        "    cls = self.get_final_output(pred,self.unique_intent)\r\n",
        "    print('intent is :',cls)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6w0PoqENHN0-",
        "outputId": "46a82a30-50bc-4eff-d263-56ff7de859c6"
      },
      "source": [
        "#object creation\r\n",
        "A = IntentClassifier()\r\n",
        "\r\n",
        "#train model\r\n",
        "A.train_model(filename = 'data/data_full.json')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data preparation begins\n",
            "keys :\n",
            "oos_val\n",
            "val\n",
            "train\n",
            "oos_test\n",
            "test\n",
            "oos_train\n",
            "{'vaccines': 1, 'cook_time': 2, 'what_is_your_name': 3, 'account_blocked': 4, 'min_payment': 5, 'translate': 6, 'restaurant_reservation': 7, 'transactions': 8, 'what_are_your_hobbies': 9, 'flight_status': 10, 'no': 11, 'international_fees': 12, 'thank_you': 13, 'confirm_reservation': 14, 'calculator': 15, 'distance': 16, 'travel_alert': 17, 'pto_balance': 18, 'income': 19, 'shopping_list': 20}\n",
            "Model Created\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 24, 128)           188672    \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 256)               263168    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 64)                16448     \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 20)                1300      \n",
            "=================================================================\n",
            "Total params: 469,588\n",
            "Trainable params: 469,588\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Training begins\n",
            "Epoch 1/10\n",
            "63/63 [==============================] - 8s 74ms/step - loss: 2.9491 - accuracy: 0.0883 - val_loss: 2.3616 - val_accuracy: 0.4475\n",
            "Epoch 2/10\n",
            "63/63 [==============================] - 4s 58ms/step - loss: 1.9180 - accuracy: 0.3943 - val_loss: 0.7942 - val_accuracy: 0.7900\n",
            "Epoch 3/10\n",
            "63/63 [==============================] - 4s 57ms/step - loss: 0.6673 - accuracy: 0.8075 - val_loss: 0.4982 - val_accuracy: 0.8475\n",
            "Epoch 4/10\n",
            "63/63 [==============================] - 4s 59ms/step - loss: 0.3410 - accuracy: 0.9183 - val_loss: 0.3580 - val_accuracy: 0.9025\n",
            "Epoch 5/10\n",
            "63/63 [==============================] - 4s 57ms/step - loss: 0.2256 - accuracy: 0.9379 - val_loss: 0.2933 - val_accuracy: 0.9225\n",
            "Epoch 6/10\n",
            "63/63 [==============================] - 4s 56ms/step - loss: 0.1018 - accuracy: 0.9772 - val_loss: 0.3305 - val_accuracy: 0.9275\n",
            "Epoch 7/10\n",
            "63/63 [==============================] - 4s 58ms/step - loss: 0.0891 - accuracy: 0.9776 - val_loss: 0.2878 - val_accuracy: 0.9425\n",
            "Epoch 8/10\n",
            "63/63 [==============================] - 4s 58ms/step - loss: 0.0616 - accuracy: 0.9874 - val_loss: 0.3228 - val_accuracy: 0.9375\n",
            "Epoch 9/10\n",
            "63/63 [==============================] - 4s 58ms/step - loss: 0.0582 - accuracy: 0.9826 - val_loss: 0.3948 - val_accuracy: 0.9175\n",
            "Epoch 10/10\n",
            "63/63 [==============================] - 4s 57ms/step - loss: 0.0575 - accuracy: 0.9827 - val_loss: 0.3937 - val_accuracy: 0.9375\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLbwZqGLH10V",
        "outputId": "2e98d4f0-e3dd-444f-aeb8-da89bbd591de"
      },
      "source": [
        "#demo with one sentence\r\n",
        "A.model_demo('how much time does it take to cook rice?')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "intent is : cook_time\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ZRIIn9oITgK",
        "outputId": "57154af8-225e-4b4d-be40-5390043abbb5"
      },
      "source": [
        "#calculate test accuracy\r\n",
        "A.evaluate_test()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test accuracy 0.9316666666666666\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBg02TGNhiZm"
      },
      "source": [
        ""
      ],
      "execution_count": 6,
      "outputs": []
    }
  ]
}