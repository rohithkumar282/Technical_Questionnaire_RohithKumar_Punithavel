{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Q1_favorite project.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHZzVSH5Zjxd"
      },
      "source": [
        "**Rohith Kumar Punithavel**\r\n",
        "\r\n",
        "**rpunitha@asu.edu**\r\n",
        "\r\n",
        "**+14802413019**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQ8LN9xkOEDT"
      },
      "source": [
        "1. Tell us about your favorite machine learning project (ideally text-based but can be any type of signal) that you’ve worked on in the past:\r\n",
        "\r\n",
        "What were the goals and focuses of this project?\r\n",
        "\r\n",
        "Describe the technical details related to the project, such as: the input features; model architectures; algorithms; metrics; optimizers; performance evaluation; etc…\r\n",
        "\r\n",
        "What were some novel approaches that you employed while solving the problem?\r\n",
        "\r\n",
        "What kinds of results did you produce?\r\n",
        "\r\n",
        "What would you change about this project? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEMJycFCNdmP"
      },
      "source": [
        "***1. Tell us about your favorite machine learning project (ideally text-based but can be any type of signal) that you’ve worked on in the past:***\r\n",
        "\r\n",
        "My most favorite text based machine learning project is Text-Visual Question Answering. I worked on this project for a course called CSE 576: Natural Language Processing in Spring 2020 at Arizona State University.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nrfRE8yOBxk"
      },
      "source": [
        "***1.1 What were the goals and focuses of this project?***\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "The main goal of the project is to provide a most reliable state of the art visual question answering model to the users. These kind of models have a wide range of application in education, assisting visually challenged people,etc,.  The main focus of this project is to behave as a third eye or a support for the people in need."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HpPTP__P5nY"
      },
      "source": [
        "***1.2 Describe the technical details related to the project, such as: the input features; model architectures; algorithms; metrics; optimizers; performance evaluation; etc…***\r\n",
        "\r\n",
        "\r\n",
        "**1.2.1 input features:**\r\n",
        "  \r\n",
        "    1.2.1.1 Question text - encoded using GloVe embeddings\r\n",
        "  \r\n",
        "    1.2.1.2 Image - 2D pixel values\r\n",
        "\r\n",
        "**1.2.2 Model Architecture -**\r\n",
        "  \r\n",
        "  1.2.2.1 Instance Module - \r\n",
        "  \r\n",
        "    1.2.2.1.1 Mask RCNN - to segment and recognize objects in the image.\r\n",
        "  \r\n",
        "  1.2.2.2 LoRRA Pythia model\r\n",
        "  \r\n",
        "    1.2.2.2.1 VQA model- to read and reason answers to question based on image.\r\n",
        "  \r\n",
        "    1.2.2.2.2 OCR module - to read text in the image.\r\n",
        "  \r\n",
        "    1.2.2.2.3 Answering module - chooses answers either from VQA, OCR or Instance module.\r\n",
        "    \r\n",
        "    1.2.2.2.4 Spell correction and n-grams - to perform spell correction and n-grams to OCR tokens.\r\n",
        "\r\n",
        "**1.2.3 Algorithm**\r\n",
        "\r\n",
        "    1.2.3.1 Get image and text from user.\r\n",
        "    1.2.3.2 The obtained image is sent through OCR, Mask RCNN module.\r\n",
        "    1.2.3.3 The OCR tokens are embedded using FastText embeddings and their spatial location is retrieved.\r\n",
        "    1.2.3.4 The labels of detections of Mask RCNN instance module is embedded with GloVe vectors and the spatial location as well is retrieved.\r\n",
        "    1.2.3.5 The question text obtained is embedded with GloVe embedding.\r\n",
        "    1.2.3.6 The image features and question embeddings are sent through attention module and the result is combined with question embedding using combination module\r\n",
        "    1.2.3.7 The OCR tokens and question embeddings are sent through another attention module and the result is combined with question embedding using another combination module.\r\n",
        "    1.2.3.8 The spatial features of text are given along with image features.\r\n",
        "    1.2.3.9. The result of 1.2.3.6 and 1.2.3.7 are combined and given as input to MLP classifier.\r\n",
        "    1.2.3.10. The OCR tokens and image features are added to a copy module.\r\n",
        "    1.2.3.11. The N answer space is expanded by M (OCR tokens and image features), thus expanding the resultant answer space to N+M.\r\n",
        "    1.2.3.12. Based on probability calculation one of N+M options are chosen. If probability goes beyond N, one of the most relevant option from the copy module is added to the answer.\r\n",
        "\r\n",
        "**1.2.4 Metrics**\r\n",
        "    \r\n",
        "    1.2.4.1 It used inter human variability in evaluating the answers.\r\n",
        "\r\n",
        "**1.2.5 Optimizers**\r\n",
        "\r\n",
        "    1.2.5.1 AdaMax optimizer was used.\r\n",
        "\r\n",
        "**1.2.6 Dataset Used**\r\n",
        "\r\n",
        "    1.2.6.1 Text VQA dataset having 28,408 images from OpenImages, 45,336 questions and 453,360 ground truth answers.\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hjxP-vISLGi"
      },
      "source": [
        "***1.3 What were some novel approaches that you employed while solving the problem?***\r\n",
        "\r\n",
        "We integrated n-grams, spell correction and labels of instance module with the existing module. In addition to that we also added spatial information of text and images. After adding all these the model was again completely retrained and evaluated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0kZSta-QClZ"
      },
      "source": [
        "***1.4 What kinds of results did you produce?***\r\n",
        "\r\n",
        "The primary challenge of this project was finding resources to train and the time, being the begining of pandemic it was quite challenging to work remotely as well. \r\n",
        "\r\n",
        "The results were slightly better than the existing module, had we trained it over the entire dataset the performace would have increased significantly. The performance improved by 0.08% and the ouput produced is either one word or combination of 2 words. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THp8mFiqQKiI"
      },
      "source": [
        "***1.5 What would you change about this project?***\r\n",
        "\r\n",
        "The main challenging part was using spatial locations of images and n-grams. I would modify the attention of spatial location in the current paper and would replace with graph based attention for spatial locations to better work with neighbouring locations. In addition to this, I will also train embeddings with attention as the model loses its performance when it comes to use of n-grams result in the answer space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZY6BrFroPzJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}