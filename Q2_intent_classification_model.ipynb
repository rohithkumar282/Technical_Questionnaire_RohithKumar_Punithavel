{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Q2_intent_classification_model.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XpZeNe-FOPpc",
        "outputId": "5d1aa158-0bb5-4898-8bc0-b292942c494f"
      },
      "source": [
        "#clone the dataset repo\r\n",
        "!git clone https://github.com/clinc/oos-eval.git\r\n",
        "%cd oos-eval"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'oos-eval'...\n",
            "remote: Enumerating objects: 64, done.\u001b[K\n",
            "remote: Counting objects: 100% (64/64), done.\u001b[K\n",
            "remote: Compressing objects: 100% (43/43), done.\u001b[K\n",
            "remote: Total 64 (delta 33), reused 48 (delta 19), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (64/64), done.\n",
            "/content/oos-eval\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6RVA0kBnAnHn",
        "outputId": "165b78b1-151f-4689-eb58-1796ee0d5e84"
      },
      "source": [
        "#import necessary packages\r\n",
        "\r\n",
        "import json\r\n",
        "from random import randint\r\n",
        "import numpy as np\r\n",
        "import nltk\r\n",
        "import string\r\n",
        "import re\r\n",
        "import os\r\n",
        "import pandas as pd\r\n",
        "from sklearn.utils import shuffle\r\n",
        "from nltk.stem import WordNetLemmatizer\r\n",
        "from nltk.tokenize import word_tokenize\r\n",
        "from keras.preprocessing.text import Tokenizer\r\n",
        "from keras.preprocessing.sequence import pad_sequences\r\n",
        "from sklearn.preprocessing import OneHotEncoder\r\n",
        "from keras.utils import to_categorical\r\n",
        "from keras.models import Sequential, load_model\r\n",
        "from keras.layers import Dense, LSTM, Bidirectional, Embedding, Dropout,BatchNormalization\r\n",
        "from keras.callbacks import ModelCheckpoint\r\n",
        "from random import seed\r\n",
        "\r\n",
        "nltk.download('wordnet')\r\n",
        "nltk.download('punkt')\r\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gNEwEKApOVxj",
        "outputId": "00fdd50e-9f75-478b-be1a-2504ec410153"
      },
      "source": [
        "#read the json file\r\n",
        "current_directory = os.getcwd()\r\n",
        "file_directory = current_directory+'/data/data_full.json'\r\n",
        "\r\n",
        "with open(file_directory) as f:\r\n",
        "  data = json.load(f)\r\n",
        "\r\n",
        "#classes in the json file\r\n",
        "for key in data.keys():\r\n",
        "  print(key)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "oos_val\n",
            "val\n",
            "train\n",
            "oos_test\n",
            "test\n",
            "oos_train\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mj72XvByOurj"
      },
      "source": [
        "#seed random number generator\r\n",
        "#seed(1)\r\n",
        "\r\n",
        "random_20_classes = []\r\n",
        "i = 0\r\n",
        "# generate 20 random integer values\r\n",
        "while i!=20:\r\n",
        "  value = randint(0, 149)\r\n",
        "  if value not in random_20_classes:\r\n",
        "    random_20_classes.append(value)\r\n",
        "    i = i+1"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpSZ0HAaR4pC"
      },
      "source": [
        "#get the data for train, test and val\r\n",
        "train_data = {'text':[],'intent':[]}\r\n",
        "test_data = {'text':[],'intent':[]}\r\n",
        "val_data = {'text':[],'intent':[]}\r\n",
        "\r\n",
        "for ele in random_20_classes:\r\n",
        "  for i in range(ele*100,ele*100+100):\r\n",
        "    train_data['text'].append(data['train'][i][0])\r\n",
        "    train_data['intent'].append(data['train'][i][1])\r\n",
        "  for i in range(ele*30,ele*30+30):\r\n",
        "    test_data['text'].append(data['test'][i][0])\r\n",
        "    test_data['intent'].append(data['test'][i][1])\r\n",
        "  for i in range(ele*20,ele*20+20):\r\n",
        "    val_data['text'].append(data['val'][i][0])\r\n",
        "    val_data['intent'].append(data['val'][i][1])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H81G41RzR6Tx"
      },
      "source": [
        "#load dataset as X and Y values \r\n",
        "def load_dataset(load_data):\r\n",
        "  load_data = pd.DataFrame(load_data)\r\n",
        "  X = load_data['text']\r\n",
        "  Y = load_data['intent']\r\n",
        "  unique_intent = list(set(Y))\r\n",
        "  return (X,Y,unique_intent)\r\n",
        "\r\n",
        "#text cleaning step - remove punctuation, tokenize and lemmatize\r\n",
        "def cleaning(sentences):\r\n",
        "  words = []\r\n",
        "  for s in sentences:\r\n",
        "    clean = \"\".join([i.lower() for i in s if i not in string.punctuation])\r\n",
        "    w = word_tokenize(clean)\r\n",
        "    #lemmatizing\r\n",
        "    words.append([lemmatizer.lemmatize(i) for i in w])\r\n",
        "  return words\r\n",
        "\r\n",
        "#create encodings\r\n",
        "def create_tokenizer(words, oov,filters = '!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~'):\r\n",
        "  if oov:\r\n",
        "    token = Tokenizer(filters = filters,oov_token='OOV')\r\n",
        "  else:\r\n",
        "    token = Tokenizer(filters=filters)\r\n",
        "  token.fit_on_texts(words)\r\n",
        "  return token\r\n",
        "\r\n",
        "#getting maximum length\r\n",
        "def max_length_fn(words):\r\n",
        "  return(len(max(words, key = len)))\r\n",
        "\r\n",
        "#encoding list of words\r\n",
        "def encoding_doc(token, words):\r\n",
        "  return(token.texts_to_sequences(words))\r\n",
        "\r\n",
        "#pad the sequence\r\n",
        "def padding_doc(encoded_doc, max_length):\r\n",
        "  return(pad_sequences(encoded_doc, maxlen = max_length,padding = \"post\"))\r\n",
        "\r\n",
        "#one hot encoding\r\n",
        "def one_hot(encode):\r\n",
        "  o = OneHotEncoder(sparse = False)\r\n",
        "  return(o.fit_transform(encode))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "paX_bO52jGz4"
      },
      "source": [
        "# call load dataset to create dataset\r\n",
        "X_train,Y_train,unique_intent = load_dataset(train_data)\r\n",
        "X_test,Y_test,_ = load_dataset(test_data)\r\n",
        "X_val,Y_val,_ = load_dataset(val_data)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bCSetRTf69a6",
        "outputId": "d884f924-d46b-4998-a0b5-4ab41e08a646"
      },
      "source": [
        "#20 distinct intents\r\n",
        "print(unique_intent)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['transactions', 'order', 'date', 'insurance', 'lost_luggage', 'change_volume', 'yes', 'how_busy', 'direct_deposit', 'next_song', 'shopping_list', 'insurance_change', 'rewards_balance', 'time', 'what_are_your_hobbies', 'how_old_are_you', 'book_hotel', 'are_you_a_bot', 'next_holiday', 'order_status']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXQgj4fh7fW9"
      },
      "source": [
        "#clean train and val\r\n",
        "Xtrain_clean = cleaning(X_train)\r\n",
        "Xval_clean = cleaning(X_val)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdCIj3lT8YsJ",
        "outputId": "ccafcfc2-7ee7-4eea-fb2e-357e0cfa161d"
      },
      "source": [
        "#tokenizer\r\n",
        "word_tokenizer = create_tokenizer(Xtrain_clean,oov=True)\r\n",
        "#vocab_size\r\n",
        "vocab_size = len(word_tokenizer.word_index) + 1\r\n",
        "#max_length calculation\r\n",
        "max_length = max_length_fn(Xtrain_clean)\r\n",
        "print(\"Vocab Size = %d and Maximum length = %d\" % (vocab_size, max_length))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab Size = 1222 and Maximum length = 28\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSJGP3AF8sRZ"
      },
      "source": [
        "#encode train and val sentences\r\n",
        "encoded_input_train = encoding_doc(word_tokenizer, Xtrain_clean)\r\n",
        "encoded_val = encoding_doc(word_tokenizer,Xval_clean)\r\n",
        "#pad train and val sentences\r\n",
        "padded_train = padding_doc(encoded_input_train, max_length)\r\n",
        "padded_val = padding_doc(encoded_val,max_length)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5rqumFD936I",
        "outputId": "3acbe970-ab37-4741-950c-4ef243ffaf3b"
      },
      "source": [
        "#create encodings for output intent class\r\n",
        "output_tokenizer = create_tokenizer(unique_intent, oov = False,filters = '!\"#$%&()*+,-/:;<=>?@[\\]^`{|}~')\r\n",
        "print(output_tokenizer.word_index)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'transactions': 1, 'order': 2, 'date': 3, 'insurance': 4, 'lost_luggage': 5, 'change_volume': 6, 'yes': 7, 'how_busy': 8, 'direct_deposit': 9, 'next_song': 10, 'shopping_list': 11, 'insurance_change': 12, 'rewards_balance': 13, 'time': 14, 'what_are_your_hobbies': 15, 'how_old_are_you': 16, 'book_hotel': 17, 'are_you_a_bot': 18, 'next_holiday': 19, 'order_status': 20}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-fNlHIs-HtY"
      },
      "source": [
        "#encode output labels for train and val\r\n",
        "encoded_output_train = encoding_doc(output_tokenizer, Y_train)\r\n",
        "encoded_output_val = encoding_doc(output_tokenizer,Y_val)\r\n",
        "\r\n",
        "#reshape vector\r\n",
        "encoded_output_train = np.array(encoded_output_train).reshape(len(encoded_output_train), 1)\r\n",
        "encoded_output_val = np.array(encoded_output_val).reshape(len(encoded_output_val), 1)\r\n",
        "\r\n",
        "#one hot encode Y for train and val\r\n",
        "output_onehot_train = one_hot(encoded_output_train)\r\n",
        "output_onehot_val = one_hot(encoded_output_val)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCrZptAqDKH2",
        "outputId": "11b77ea8-3d33-49c1-ad47-c6ec4982caaf"
      },
      "source": [
        "#create model\r\n",
        "def create_model(vocab_size, max_length):\r\n",
        "  model = Sequential()\r\n",
        "  model.add(Embedding(vocab_size, 128, input_length = max_length))\r\n",
        "  model.add(Bidirectional(LSTM(128)))\r\n",
        "  model.add(Dense(64, activation = \"relu\"))\r\n",
        "  model.add(Dropout(0.4))\r\n",
        "  model.add(Dense(20, activation = \"softmax\"))\r\n",
        "  return model\r\n",
        "\r\n",
        "model = create_model(vocab_size, max_length)\r\n",
        "#optimizer, metrics, and loss\r\n",
        "model.compile(loss = \"categorical_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\r\n",
        "model.summary()\r\n",
        "#checkpoint for model\r\n",
        "checkpoint = ModelCheckpoint('intent_classification_model.h5', verbose=0, save_best_only=True, mode='auto',save_freq='epoch')\r\n",
        "#shuffle data\r\n",
        "train_X, train_Y = shuffle(padded_train,output_onehot_train)\r\n",
        "val_X,val_Y = shuffle(padded_val,output_onehot_val)\r\n",
        "\r\n",
        "#model training\r\n",
        "model.fit(train_X, train_Y, epochs = 10, batch_size = 32, validation_data = (val_X, val_Y), callbacks = [checkpoint])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 28, 128)           156416    \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 256)               263168    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 64)                16448     \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 20)                1300      \n",
            "=================================================================\n",
            "Total params: 437,332\n",
            "Trainable params: 437,332\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "63/63 [==============================] - 11s 27ms/step - loss: 2.9702 - accuracy: 0.0745 - val_loss: 2.5986 - val_accuracy: 0.2425\n",
            "Epoch 2/10\n",
            "63/63 [==============================] - 1s 11ms/step - loss: 2.1366 - accuracy: 0.3312 - val_loss: 1.1024 - val_accuracy: 0.7400\n",
            "Epoch 3/10\n",
            "63/63 [==============================] - 1s 11ms/step - loss: 0.9428 - accuracy: 0.6944 - val_loss: 0.6416 - val_accuracy: 0.8800\n",
            "Epoch 4/10\n",
            "63/63 [==============================] - 1s 11ms/step - loss: 0.4109 - accuracy: 0.8921 - val_loss: 0.4519 - val_accuracy: 0.8875\n",
            "Epoch 5/10\n",
            "63/63 [==============================] - 1s 11ms/step - loss: 0.2090 - accuracy: 0.9528 - val_loss: 0.4642 - val_accuracy: 0.8950\n",
            "Epoch 6/10\n",
            "63/63 [==============================] - 1s 11ms/step - loss: 0.1520 - accuracy: 0.9573 - val_loss: 0.4238 - val_accuracy: 0.9125\n",
            "Epoch 7/10\n",
            "63/63 [==============================] - 1s 11ms/step - loss: 0.1349 - accuracy: 0.9690 - val_loss: 0.4212 - val_accuracy: 0.9275\n",
            "Epoch 8/10\n",
            "63/63 [==============================] - 1s 12ms/step - loss: 0.0767 - accuracy: 0.9818 - val_loss: 0.4382 - val_accuracy: 0.9100\n",
            "Epoch 9/10\n",
            "63/63 [==============================] - 1s 11ms/step - loss: 0.0608 - accuracy: 0.9848 - val_loss: 0.4905 - val_accuracy: 0.9150\n",
            "Epoch 10/10\n",
            "63/63 [==============================] - 1s 11ms/step - loss: 0.0576 - accuracy: 0.9855 - val_loss: 0.5841 - val_accuracy: 0.8825\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f61c0353470>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uyHFMNg2RTgv",
        "outputId": "a83d2abf-0ea7-467f-eb7e-c8bdc4c26012"
      },
      "source": [
        "#load best model\r\n",
        "model = load_model(\"intent_classification_model.h5\")\r\n",
        " \r\n",
        "#calculate predictions \r\n",
        "def predictions(text):\r\n",
        "  test_word = cleaning(text)\r\n",
        "  test_ls = encoding_doc(word_tokenizer,test_word)\r\n",
        "  x = padding_doc(test_ls, max_length)\r\n",
        "  pred = model.predict(x)\r\n",
        "  return pred\r\n",
        "\r\n",
        "#calculate classes\r\n",
        "def get_final_output(pred, classes):\r\n",
        "  predictions = pred[0]\r\n",
        "  classes = np.array(classes)\r\n",
        "  ids = np.argsort(-predictions)\r\n",
        "  classes = classes[ids]\r\n",
        "  predictions = -np.sort(-predictions)\r\n",
        "  return classes[0]\r\n",
        "\r\n",
        "#calculate test accuracy\r\n",
        "count = 0\r\n",
        "for i in range(len(X_test)):\r\n",
        "  pred = predictions([X_test[i]])\r\n",
        "  cls = get_final_output(pred, unique_intent)\r\n",
        "  if cls==Y_test[i]:\r\n",
        "    count=count+1\r\n",
        "print('test accuracy ',count/len(Y_test))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test accuracy  0.9333333333333333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbPhTe5XrGyX"
      },
      "source": [
        "**Describe the process involved with deciding on the input feature representation, the model architecture, the training-related parameters, the performance evaluation metric(s), etc. Also, discuss challenges, simplifications made, and future work.**\r\n",
        "\r\n",
        "**Ans:**\r\n",
        "\r\n",
        "1. The dataset is read and using random number generator, random numbers are generated to choose 20 distinct in scope intent classes.\r\n",
        "\r\n",
        "2. The respective train, val, and test data is retrieved for 20 different in scope intent classes.\r\n",
        "\r\n",
        "3. This is a supervised learning task with text as data and intent as labels.\r\n",
        "\r\n",
        "4. There are 100 data in train, 20 in validation and 30 in test for each in scope intent classes.\r\n",
        "\r\n",
        "5. The data is split into X and Y based on text and labels for each class.\r\n",
        "\r\n",
        "6. Then the text data is cleaned:\r\n",
        "\r\n",
        "  6.1 The punctuations are removed first and converted to lower case.\r\n",
        "\r\n",
        "  6.2 The remaining words are the tokenized.\r\n",
        "\r\n",
        "  6.3 The tokenized words are then lemmatized.\r\n",
        "\r\n",
        "\r\n",
        "  The reason for going for lemmatizing than stemming is, since being an intent classification class the meaning of words are really important. When these are stemmed then there is a high probability for the meaning of the word to change. In this task stopwords are not removed, being a small dataset if the stop words are removed the model will not be able to learn more relationships in texts, hence the stopwords are not removed.\r\n",
        "\r\n",
        "7. The cleaned data is then tokenized, that is a dictionary or hash map of numbers is created which is used for encoding the text. This also determines the vocab_size.\r\n",
        "\r\n",
        "   The encodings are generated only for data in the training set and not including the validation or test set. The main purpose of using validation or test set is to see how model performs on the data that it has not seen during the training. Since in most applications the data is split and checked but in this case the split data is given on hand and hence the val and test data is used for evaluation purpose alone.\r\n",
        "\r\n",
        "8. Then the sentence texts are encoded and out of vocabulary words are not omitted.\r\n",
        "\r\n",
        "9. Then the encodings are padded with zero at the end to match the maximum length of sequence of training data.\r\n",
        "\r\n",
        "10. The intent classes are then tokenized and assigned encoding values. Then they are one hot encoded.\r\n",
        "\r\n",
        "11. After the data for X and Y is prepared, the next step is model creation.\r\n",
        "\r\n",
        "12. Being a time constraint assessment, I created a model with one Bidirectional LSTM layer, one dense layer and one final softmax layer with 20 units. Dropout layer is added to avoid overfitting of the model.\r\n",
        "\r\n",
        "13. The model is trained and optimized with Adam optimizer, the loss is categorical crossentropy with the metrics 'accuracy'.\r\n",
        "\r\n",
        "14. The model is trained for 10 epochs, with batch size 32 and only the best model is saved.\r\n",
        "\r\n",
        "15. Then the saved best model is then used to estimate accuracy of the test set.\r\n",
        "\r\n",
        "\r\n",
        "**Challenges:**\r\n",
        "1. The main challenge in this task is to decide what kind of preprocessing steps needs to be employed.\r\n",
        "2. Being a small dataset effective model design and hyperparamets are necessary.\r\n",
        "\r\n",
        "**Simplification made**\r\n",
        "1. Being a time constrained task, I spent a major time in cleaning the dataset and getting the data in the right format. Hence, I had to compromise on going with a simple model and not a deep or complex model.\r\n",
        "2. Being a small model, CPU alone was neede to train the model.\r\n",
        "\r\n",
        "**Future Work**\r\n",
        "1. Include all the data for 150 classes and remove stopwords to minimize the vocab size.\r\n",
        "2. Use attention models to learn embeddings and perform classification task which will give a better result once the stopwords are removed. \r\n",
        "3. Use deep model to learn more relationships among words.\r\n"
      ]
    }
  ]
}